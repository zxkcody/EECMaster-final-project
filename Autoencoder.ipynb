{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional autoencoder for image reconstruction\n",
    "# Adapted from https://keras.io/examples/vision/autoencoder/#build-the-autoencoder\n",
    "\n",
    "## set up\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras import layers\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Model\n",
    "from PIL import Image\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Reshape\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays corresponding images from each one of the supplied arrays side by side.\n",
    "    \"\"\"\n",
    "\n",
    "    n = 10\n",
    "\n",
    "    indices1 = np.random.randint(len(array1), size=n)\n",
    "    indices2 = np.random.randint(len(array2), size=n)\n",
    "    images1 = array1[indices1, :]\n",
    "    images2 = array2[indices2, :]\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(images1[i].reshape(224, 224))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(images2[i].reshape(224, 224))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "input_img = Input(shape=(224, 224, 1))\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "flatten = Flatten()(x)\n",
    "encoded = Dense(3136, activation='relu')(flatten)  \n",
    "\n",
    "middle_layer = encoded\n",
    "\n",
    "# Decoder\n",
    "x = Dense(3136, activation='relu')(encoded)\n",
    "reshape = Reshape((28, 28, 4))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(reshape)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all segmented png images\n",
    "folder_path = '/home/xzhu/517/Toarcian AI project inc ox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the input images\n",
    "input_shape = (224, 224, 1)  # For grayscale images\n",
    "\n",
    "# Get a list of all the image files in the directory\n",
    "image_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.png')]\n",
    "\n",
    "# Calculate the number of images\n",
    "num_images = len(image_files)\n",
    "\n",
    "# Create an empty NumPy array to store images\n",
    "images = np.empty((num_images, *input_shape), dtype=np.uint8)\n",
    "\n",
    "# Load and preprocess images\n",
    "for i, filename in enumerate(image_files):\n",
    "    image_path = os.path.join(folder_path, filename)\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    image = image.resize((input_shape[0], input_shape[1]))\n",
    "    image_array = np.array(image)\n",
    "    images[i] = image_array.reshape(input_shape)\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "images = images.astype('float32') / 255.0\n",
    "\n",
    "# Split the images into training and test sets\n",
    "train_data, test_data = train_test_split(images, test_size=0.2, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAE = autoencoder.fit(\n",
    "    x=train_data,\n",
    "    y=train_data,\n",
    "    epochs= 50,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(test_data, test_data),\n",
    "    callbacks=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(test_data)\n",
    "display(train_data, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = CAE.history['loss']\n",
    "val_loss = CAE.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, val_loss, 'b-')\n",
    "plt.title('Autoencoder Model Loss')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('autoencoder_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('autoencoder_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory = '/home/xzhu/517/test'\n",
    "image_size = (224, 224) \n",
    "\n",
    "# Get a list of all the image files in the directory\n",
    "image_files = [filename for filename in os.listdir(directory) if filename.endswith('.png')]\n",
    "\n",
    "# Create an empty list to store the images\n",
    "image_list = []\n",
    "\n",
    "# Iterate over the PNG files and read each image\n",
    "for file in image_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    image = cv2.imread(file_path)\n",
    "\n",
    "    image = cv2.resize(image, image_size)\n",
    "\n",
    "    # Convert image into binary image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Append the resized image to the list\n",
    "    image_list.append(image)\n",
    "\n",
    "# Convert the list of images to a NumPy array\n",
    "image_array = np.array(image_list)\n",
    "\n",
    "# Print the shape of the image array\n",
    "print(image_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer('dense').output)\n",
    "middle_layer = encoder.predict(image_array)\n",
    "print(middle_layer.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
